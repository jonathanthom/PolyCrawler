# Is crawl resuming feature enabled?
# If this feature is enabled, you would be able to 
# resume a previously stopped/crashed crawl. However,
# it makes crawling slightly slower
crawler.enable_resume=false

# Maximum depth of crawling
# For unlimited depth this parameter should be set to -1
crawler.max_depth=-1

# Maximum number of pages to fetch
# For unlimited number of pages, this parameter should be set to -1
crawler.max_pages_to_fetch=-1

# Should the crawler obey Robots.txt protocol?
# More info on Robots.txt is available at http://www.robotstxt.org/
crawler.obey_robotstxt=true

# Should we fetch binary content such as images, audio, ...?
crawler.include_binary_content=false

# Default encoding of pages
crawler.default_encoding=UTF-8

# Maximum Connections per host
fetcher.max_connections_per_host=100

# Maximum total connections
fetcher.max_total_connections=100

# Socket timeout in milliseconds
fetcher.socket_timeout=20000

# Connection timeout in milliseconds
fetcher.connection_timeout=30000

# Max number of outgoing links which are processed from a page
fetcher.max_outlinks=5000

# Max size of page is 1Mb
fetcher.max_download_size=1048576

# user-agent
fetcher.user_agent=crawler4j (http://code.google.com/p/crawler4j/)
fetcher.user_agent_name=crawler4j

# Default politeness delay in milliseconds
fetcher.default_politeness_delay=200

# Should we also crawl https pages?
fetcher.crawl_https=false

# Should we follow redirects?
fetcher.follow_redirects=true

# Should we log the 404 (Not Found) pages?
logging.show_404_pages=false